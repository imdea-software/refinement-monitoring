%!TEX root = head.tex

\section{Introduction}
\label{sec:intro}

Efficient implementations of concurrent objects such as semaphores, locks, and
atomic collections including stacks and queues are vital to modern computer
systems. Programming them is however error prone. To minimize synchronization
overhead between concurrent object-method invocations, implementors avoid
blocking operations like lock acquisition, allowing methods to execute
concurrently. However, concurrency risks unintended inter-operation
interference, and risks conformance to reference implementations. Conformance
is formally captured by \emph{observational refinement}: given two libraries
$L_1$ and $L_2$ implementing the methods of some concurrent object, we say
$L_1$ \emph{refines} $L_2$ if and only if every computation of every program
using $L_1$ would also be possible were $L_2$ used instead.

Verifying observational refinement is intrinsically hard: it is undecidable
even for finite-state implementations whose methods can be called concurrently
by arbitrarily-many threads~\cite{conf/esop/BouajjaniEEH13}. In fact, even
checking the conformance of a single program execution using one library with
respect to another is intractable~\cite{journals/siamcomp/GibbonsK97}. In
practice, fully-automated techniques for checking observational refinement have
been limited to detecting violations in executions with very few library-method
invocations.

In this work we develop a fully-automated and highly-scalable means of
detecting violations to observational refinement by monitoring program
executions. The key challenge we address is how to achieve scalability while
maintaining precision/completeness. Detecting refinement violations may require
observing large executions, with many \emph{operations}, i.e.,~object-method
invocations. However, the complexity of precise violation-checking is
exponential in the number of operations. Essentially, this check amounts to
considering every possible \emph{linearization} of an execution's operations,
which are only partially-ordered by their happens-before relation. Only if none
of the linearizations represent a valid sequence of method invocations does the
execution witness a violation.

Our approach is based on sound yet possibly-incomplete means for avoiding the
practical scalability pitfalls, the most immediate pitfall being the explicit
enumeration of possible linearizations. We discover that naturally-occurring
concurrent objects, including atomic collections, locks, and semaphores, can be
expressed symbolically, in a first-order language over their method names,
argument/return values, and invocation-ordering constraints. Ultimately this
allows us to reduce violation detection for single executions to satisfiability
in propositional logic. Practically speaking, this allows us to exploit the
highly-developed algorithms of modern symbolic reasoning engines in place of
the explicit enumeration of linearizations. Furthermore, symbolic reasoning
lends itself to \emph{incrementality}: as the symbolic representation of each
successive execution step differs monotonically, only by the addition of a new
operation or return value, we can reuse all logical implications of previous
steps. Conceptually, this avoids recomputing the set of possible linearizations
from scratch after each execution step.

While exploiting symbolic reasoning engines makes sense practically, and is
likely to be more efficient, the link to symbolic reasoning also reveals
insights leading to more-drastic optimizations. In particular, we notice that
in proving satisfiability, the solver must essentially build a model of some
linearization of the given execution which represents a valid object
method-invocation sequence. In doing so, the solver may need to make branching
decisions in addition to logical deductions, or \emph{unit propagation},
possibly backtracking later, about (a)~how pending operations should be
completed/dropped, and (b)~which operations should be linearized before others.
However, from the perspective of detecting violations, it is always sound to
forgo such costly branching/backtracking and, for instance, wait until the
given pending operations are actually completed later in the execution to
determine their return values. Though it is unclear whether such strategies
might actually be complete in theory, we hypothesize that they are effective in
practice, uncovering most violations.

Though limiting symbolic reasoning to saturation, or unit propagation, does
avoid the exponential cost in the number of operations, a truly useful runtime
monitor for observational refinement ought to be \emph{linear} in the number of
operations, and incur only a constant space overhead; otherwise it will
progressively retard program execution and/or eventually exhaust program
memory. Achieving this complexity goal implies that the monitor cannot store
all previously-executed operations. However, simply forgetting arbitrary
operations is unsound, in the sense that we may conclude a violation when none
actually occurred. For instance, if a monitor for an atomic queue object
observes ordered ${\sf enqueue}(a)$ and ${\sf enqueue}(b)$ operations, and
${\sf dequeue}(b)$, having dropped ${\sf dequeue}(a)$, the monitor may detect a
violation, thinking ${\sf dequeue}(b)$ should not precede ${\sf dequeue}(a)$.
To resolve this issue, we develop a sound theory for the removal of
\emph{matched} operations, e.g.,~the {\sf enqueue} operations together with the
{\sf dequeue} operations removing the same added elements. We hypothesize that
such strategies are, too, effective in practice, in that they do not cause many
violations to be missed.

Empirically we demonstrate that our violation-detection strategies are
profoundly-more scalable than existing techniques. We also validate our
aforementioned completeness hypotheses: that in practice, most violations are
caught despite the possibility for incompleteness.

To summarize, we make the following contributions:
\begin{itemize}

  \item First-order logical characterizations of naturally-occurring concurrent
  objects, allowing for symbolic reasoning about observational refinement
  (\S\ref{sec:logic}).

  \item A reduction from refinement-violation detection for single executions
  to propositional logic satisfiability (\S\ref{sec:propositional}).
  
  \item A sound theory of matched-operation removal, allowing the scalability
  required for runtime monitoring (\S\ref{sec:obsolete}).
  
  \item Empirical demonstration that our violation-detection optimizations are
  scalable and effectively-complete (\S\ref{sec:exp}).

\end{itemize}
We begin with a formalization of observational refinement
(\S\ref{sec:refinement}), and conclude with a discussion of related work
(\S\ref{sec:related}).
